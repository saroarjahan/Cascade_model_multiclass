{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For keyword-based model development, I would try following steps, and based on each step's results, I would try  to improve model accuracy\n",
    "\n",
    "Step 1: Keyword matching First, keyword matching will be performed. If the hate keyword matches the sentence, it will return hate samples otherwise normal.\n",
    "\n",
    "Step 2: contextual keyword embedding matching with contextual post embedding\n",
    "\n",
    "\n",
    "<i><strong>Dataset will be divided into training and testing; we will use the training part dataset to build the keyword list so that model does not overfit.<strong><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset jason format and convert into pandas dataframe\n",
    "df = pd.read_json('data/dataset.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>9989999_gab</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...</td>\n",
       "      <td>[if, ur, still, on, twitter, tell, carlton, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>9990225_gab</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
       "      <td>[when, i, first, got, on, here, and, said, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>9991681_gab</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[was, macht, der, moslem, wenn, der, zion, geg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20146</th>\n",
       "      <td>9992513_gab</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...</td>\n",
       "      <td>[it, is, awful, look, at, world, demographics,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>9998729_gab</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
       "      <td>[the, jewish, globalist, elite, have, only, im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20148 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           post_id  annotators  \\\n",
       "0      1179055004553900032_twitter           0   \n",
       "1      1179063826874032128_twitter           0   \n",
       "2      1178793830532956161_twitter           0   \n",
       "3      1179088797964763136_twitter           1   \n",
       "4      1179085312976445440_twitter           1   \n",
       "...                            ...         ...   \n",
       "20143                  9989999_gab           1   \n",
       "20144                  9990225_gab           1   \n",
       "20145                  9991681_gab           1   \n",
       "20146                  9992513_gab           1   \n",
       "20147                  9998729_gab           1   \n",
       "\n",
       "                                              rationales  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2                                                     []   \n",
       "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "...                                                  ...   \n",
       "20143  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...   \n",
       "20144  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n",
       "20145                                                 []   \n",
       "20146  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...   \n",
       "20147  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n",
       "\n",
       "                                             post_tokens  \n",
       "0      [i, dont, think, im, getting, my, baby, them, ...  \n",
       "1      [we, cannot, continue, calling, ourselves, fem...  \n",
       "2                    [nawt, yall, niggers, ignoring, me]  \n",
       "3      [<user>, i, am, bit, confused, coz, chinese, p...  \n",
       "4      [this, bitch, in, whataburger, eating, a, burg...  \n",
       "...                                                  ...  \n",
       "20143  [if, ur, still, on, twitter, tell, carlton, i,...  \n",
       "20144  [when, i, first, got, on, here, and, said, i, ...  \n",
       "20145  [was, macht, der, moslem, wenn, der, zion, geg...  \n",
       "20146  [it, is, awful, look, at, world, demographics,...  \n",
       "20147  [the, jewish, globalist, elite, have, only, im...  \n",
       "\n",
       "[20148 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### step 1 key word matching\n",
    "data=df.T.reset_index().drop(['index'], axis=1)\n",
    "data['annotators'] = data['annotators'].apply((lambda x: x[0]['label']))\n",
    "data['annotators'] = data['annotators'] .apply((lambda x: re.sub('normal','0',x)))\n",
    "data['annotators'] = data['annotators'] .apply((lambda x: re.sub('hatespeech','1',x)))\n",
    "data['annotators'] = data['annotators'] .apply((lambda x: re.sub('offensive','1',x)))\n",
    "data['annotators'] =pd.to_numeric(data['annotators'])\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mjahan18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Download the stopwords list\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beaners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mudslime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>niggers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mudshark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>muzzies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>coon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>muzzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>goyim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>whore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sucking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>inbred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sheboon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>shitskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>goat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>degenerate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>muh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>muzrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>chink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bisexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>jew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rapists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>raped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bitches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>retarded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      keywords\n",
       "0      beaners\n",
       "1       filthy\n",
       "2         kike\n",
       "3       beaner\n",
       "4       nigger\n",
       "5        kikes\n",
       "6     mudslime\n",
       "7        shill\n",
       "8         spic\n",
       "9         sand\n",
       "10     niggers\n",
       "11    mudshark\n",
       "12       spics\n",
       "13     muzzies\n",
       "14        gook\n",
       "15        coon\n",
       "16      muzzie\n",
       "17       goyim\n",
       "18       whore\n",
       "19         jew\n",
       "20     sucking\n",
       "21      inbred\n",
       "22     sheboon\n",
       "23    shitskin\n",
       "24        goat\n",
       "25  degenerate\n",
       "26        cock\n",
       "27         muh\n",
       "28       filth\n",
       "29      muzrat\n",
       "30       chink\n",
       "31    bisexual\n",
       "32         jew\n",
       "33     rapists\n",
       "34      nigger\n",
       "35       raped\n",
       "36     bitches\n",
       "37    retarded\n",
       "38       white"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = pd.read_csv('keywords/hatespeech_keywords.csv')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 keyword mathcing modle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of simple hate key word matching:  0.6036827476672623\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function that takes a text input and returns a binary label for hate speech (1 if the input contains any of the hate keywords, 0 otherwise)\n",
    "def classify_text(text):\n",
    "    text=\" \".join(text)\n",
    "    for keyword in keywords['keywords']:\n",
    "        if re.search(keyword, text, re.IGNORECASE):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Example usage:\n",
    "predictions=[]\n",
    "for sen in data['post_tokens']:\n",
    "    predictions.append(classify_text(sen))\n",
    "    \n",
    "# accuracy calculation     \n",
    "accuracy = metrics.accuracy_score(np.array(predictions),np.array(data['annotators']))\n",
    "print('Accuracy of simple hate key word matching: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of simple hate key word matching:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.51      0.57     10220\n",
      "           1       0.58      0.70      0.63      9928\n",
      "\n",
      "    accuracy                           0.60     20148\n",
      "   macro avg       0.61      0.61      0.60     20148\n",
      "weighted avg       0.61      0.60      0.60     20148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy calculation     \n",
    "accuracy = metrics.classification_report(np.array(predictions),np.array(data['annotators']))\n",
    "print('Accuracy of simple hate key word matching: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step -2\n",
    "\n",
    "Here first string matching model  provides 60%. This accuray is low and there of coule of thing should be focused, Firstly the keywords that has been provided is not adequte and some are not even hate speech keyword which can be used as not hate keyord (e.g, muslimes, jews). In this case follwowing update can be done, for example, making a hate topic model with multiple important key words. Also adding more hateful keywords in the keyword list would be helpful.\n",
    "\n",
    "#### Following updates are performaing in second steps\n",
    "1. Adding more hate keywwords and hate phrases in the keylist\n",
    "3. Calculate contectual synonyms and consine similarty with selected hate keywords or phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bert tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect hate words or phrases from dataset. Coolected only thos words which all three annotators consider as hate\n",
    "# to do that i have mapped post tokens with annotators rationalies. \n",
    "hate_words=[]\n",
    "for i in range(20000):\n",
    "    if data['annotators'][i]==1:\n",
    "        try:\n",
    "                    # Example input data\n",
    "            lists = data['rationales'][i]\n",
    "            words = data['post_tokens'][i]\n",
    "\n",
    "            # Find indices of 1s in the sublists and add corresponding words to a list\n",
    "            matches = []\n",
    "            for sublist in lists:\n",
    "                sublist_matches = [words[i] for i, x in enumerate(sublist) if x == 1]\n",
    "                matches.append(sublist_matches)\n",
    "\n",
    "            # Find common words in all sublists\n",
    "            common_words = []\n",
    "            for i in range(len(matches[0])):\n",
    "                word = matches[0][i]\n",
    "                if all(word in sublist for sublist in matches[1:]):\n",
    "                    common_words.append(word)\n",
    "                    \n",
    "\n",
    "            if len(common_words) >=1:\n",
    "                # Print the common words\n",
    "#                 filtered_words = [word for word in common_words if word.lower() not in stop_words]\n",
    "                # Join the filtered words back into a sentence\n",
    "                filtered_sentence = ' '.join(common_words)\n",
    "                \n",
    "                hate_words.append( filtered_sentence)\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4680"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplciate words form list\n",
    "hate_words = list(set(hate_words))\n",
    "hate_words = list(filter(lambda x: x != \"\", hate_words))\n",
    "len(hate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create contectual BERT embeddings of a words\n",
    "keyword_embeddings = []\n",
    "for k in hate_words:\n",
    "    encoded_input = tokenizer(k, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        embeddings = output.last_hidden_state.mean(dim=1)\n",
    "        keyword_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_roberta(embeddings1, embeddings2):\n",
    "    # Calculate cosine similarity between the two embeddings\n",
    "    similarity = torch.nn.functional.cosine_similarity(embeddings1, embeddings2, dim=1)\n",
    "    return similarity\n",
    "\n",
    "def keyword_model(post, threshold):\n",
    "    # Get embeddings for the post\n",
    "    post=\" \".join(post)\n",
    "    encoded_input = tokenizer(post, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        post_embeddings = output.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between the post and all keywords\n",
    "    total_similarity = 0\n",
    "    min_length = min(len(keyword_embeddings), len(keywords['keywords']))\n",
    "    for i in range(min_length):\n",
    "        word_count = len(keywords['keywords'][i].split())\n",
    "        similarities = torch.nn.functional.cosine_similarity(post_embeddings, keyword_embeddings[i], dim=1)\n",
    "        total_similarity += similarities.item()\n",
    "\n",
    "    # Check if the total similarity is high enough\n",
    "    if total_similarity / min_length >= threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "predictions = []\n",
    "for sen in data['post_tokens'][:2000]:\n",
    "    predictions.append(keyword_model(sen, 0.20000000000000004))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of simple hate key word matching:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.76      0.06        37\n",
      "           1       0.99      0.52      0.68      1963\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.51      0.64      0.37      2000\n",
      "weighted avg       0.97      0.53      0.67      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy calculation     \n",
    "accuracy = metrics.classification_report(np.array(predictions),np.array(data['annotators'][:2000]))\n",
    "print('Accuracy of simple hate key word matching: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(np.array(predictions),np.array(data['annotators'][:2000]))\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Normal', 'hate'])\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##finding best thresolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.20000000000000004, Best F1 score: 0.6826987307949232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# The range of thresholds to test\n",
    "thresholds = np.arange(0.1, 0.5, 0.05)\n",
    "\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Make predictions on the validation set\n",
    "    predictions = []\n",
    "    for sen in data['post_tokens'][:2000]:\n",
    "        predictions.append(keyword_model(sen, threshold))  # Add threshold as argument here\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(data['annotators'][:2000], predictions)\n",
    "\n",
    "    # Update the best threshold if this F1 score is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f'Best threshold: {best_threshold}, Best F1 score: {best_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
