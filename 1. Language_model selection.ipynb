{"cells":[{"cell_type":"code","execution_count":null,"id":"8f7c45bd","metadata":{"id":"8f7c45bd"},"outputs":[],"source":["# importing dependencies\n","import pandas as pd\n","import numpy as np\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","import keras\n","from keras.utils import np_utils\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import roc_auc_score\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import preprocessor as p"]},{"cell_type":"code","execution_count":null,"id":"7d2d8e08","metadata":{"id":"7d2d8e08"},"outputs":[],"source":["#read dataset jason format and convert into pandas dataframe\n","df = pd.read_json('data/dataset.json')\n"]},{"cell_type":"code","execution_count":null,"id":"5b8d570c","metadata":{"id":"5b8d570c","outputId":"4b543f32-d19e-4039-8c58-e0cf1c7ac8ca"},"outputs":[{"data":{"text/plain":["'This is example of tweet emoji reooval #awesome https://github.com/s/preprocessor'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#example of twwet cleam+ning\n","p.set_options(p.OPT.EMOJI, p.OPT.NUMBER)\n","p.clean('This is example of tweet emoji reooval #awesome üëç https://github.com/s/preprocessor')"]},{"cell_type":"code","execution_count":null,"id":"f1618d8c","metadata":{"id":"f1618d8c","outputId":"d6e3b962-4a56-49a2-b100-915673bcb252"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>post_id</th>\n","      <th>annotators</th>\n","      <th>rationales</th>\n","      <th>post_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1179055004553900032_twitter</td>\n","      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n","      <td>[]</td>\n","      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1179063826874032128_twitter</td>\n","      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n","      <td>[]</td>\n","      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1178793830532956161_twitter</td>\n","      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n","      <td>[]</td>\n","      <td>[nawt, yall, niggers, ignoring, me]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1179088797964763136_twitter</td>\n","      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1179085312976445440_twitter</td>\n","      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n","      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20143</th>\n","      <td>9989999_gab</td>\n","      <td>[{'label': 'offensive', 'annotator_id': 217, '...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...</td>\n","      <td>[if, ur, still, on, twitter, tell, carlton, i,...</td>\n","    </tr>\n","    <tr>\n","      <th>20144</th>\n","      <td>9990225_gab</td>\n","      <td>[{'label': 'offensive', 'annotator_id': 220, '...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n","      <td>[when, i, first, got, on, here, and, said, i, ...</td>\n","    </tr>\n","    <tr>\n","      <th>20145</th>\n","      <td>9991681_gab</td>\n","      <td>[{'label': 'offensive', 'annotator_id': 206, '...</td>\n","      <td>[]</td>\n","      <td>[was, macht, der, moslem, wenn, der, zion, geg...</td>\n","    </tr>\n","    <tr>\n","      <th>20146</th>\n","      <td>9992513_gab</td>\n","      <td>[{'label': 'hatespeech', 'annotator_id': 209, ...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...</td>\n","      <td>[it, is, awful, look, at, world, demographics,...</td>\n","    </tr>\n","    <tr>\n","      <th>20147</th>\n","      <td>9998729_gab</td>\n","      <td>[{'label': 'hatespeech', 'annotator_id': 200, ...</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n","      <td>[the, jewish, globalist, elite, have, only, im...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20148 rows √ó 4 columns</p>\n","</div>"],"text/plain":["                           post_id  \\\n","0      1179055004553900032_twitter   \n","1      1179063826874032128_twitter   \n","2      1178793830532956161_twitter   \n","3      1179088797964763136_twitter   \n","4      1179085312976445440_twitter   \n","...                            ...   \n","20143                  9989999_gab   \n","20144                  9990225_gab   \n","20145                  9991681_gab   \n","20146                  9992513_gab   \n","20147                  9998729_gab   \n","\n","                                              annotators  \\\n","0      [{'label': 'normal', 'annotator_id': 1, 'targe...   \n","1      [{'label': 'normal', 'annotator_id': 1, 'targe...   \n","2      [{'label': 'normal', 'annotator_id': 4, 'targe...   \n","3      [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n","4      [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n","...                                                  ...   \n","20143  [{'label': 'offensive', 'annotator_id': 217, '...   \n","20144  [{'label': 'offensive', 'annotator_id': 220, '...   \n","20145  [{'label': 'offensive', 'annotator_id': 206, '...   \n","20146  [{'label': 'hatespeech', 'annotator_id': 209, ...   \n","20147  [{'label': 'hatespeech', 'annotator_id': 200, ...   \n","\n","                                              rationales  \\\n","0                                                     []   \n","1                                                     []   \n","2                                                     []   \n","3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","...                                                  ...   \n","20143  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...   \n","20144  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n","20145                                                 []   \n","20146  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...   \n","20147  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n","\n","                                             post_tokens  \n","0      [i, dont, think, im, getting, my, baby, them, ...  \n","1      [we, cannot, continue, calling, ourselves, fem...  \n","2                    [nawt, yall, niggers, ignoring, me]  \n","3      [<user>, i, am, bit, confused, coz, chinese, p...  \n","4      [this, bitch, in, whataburger, eating, a, burg...  \n","...                                                  ...  \n","20143  [if, ur, still, on, twitter, tell, carlton, i,...  \n","20144  [when, i, first, got, on, here, and, said, i, ...  \n","20145  [was, macht, der, moslem, wenn, der, zion, geg...  \n","20146  [it, is, awful, look, at, world, demographics,...  \n","20147  [the, jewish, globalist, elite, have, only, im...  \n","\n","[20148 rows x 4 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#read data and make  a proper format of pandas dataframe\n","data=df.T.reset_index().drop(['index'], axis=1)\n","data"]},{"cell_type":"code","execution_count":null,"id":"57a89f37","metadata":{"id":"57a89f37","outputId":"c58f20b2-7914-40f7-82ac-79061ff43232"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>post_id</th>\n","      <th>annotators</th>\n","      <th>rationales</th>\n","      <th>post_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1179055004553900032_twitter</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>i dont think im getting my baby them white he ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1179063826874032128_twitter</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>we cannot continue calling ourselves feminists...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1178793830532956161_twitter</td>\n","      <td>0</td>\n","      <td>[]</td>\n","      <td>nawt yall niggers ignoring me</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1179088797964763136_twitter</td>\n","      <td>1</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>i am bit confused coz chinese ppl can not acce...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1179085312976445440_twitter</td>\n","      <td>1</td>\n","      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","      <td>this bitch in whataburger eating a burger with...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20143</th>\n","      <td>9989999_gab</td>\n","      <td>2</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...</td>\n","      <td>if ur still on twitter tell carlton i said his...</td>\n","    </tr>\n","    <tr>\n","      <th>20144</th>\n","      <td>9990225_gab</td>\n","      <td>2</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n","      <td>when i first got on here and said i hate trump...</td>\n","    </tr>\n","    <tr>\n","      <th>20145</th>\n","      <td>9991681_gab</td>\n","      <td>2</td>\n","      <td>[]</td>\n","      <td>was macht der moslem wenn der zion gegen seine...</td>\n","    </tr>\n","    <tr>\n","      <th>20146</th>\n","      <td>9992513_gab</td>\n","      <td>1</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...</td>\n","      <td>it is awful look at world demographics asians ...</td>\n","    </tr>\n","    <tr>\n","      <th>20147</th>\n","      <td>9998729_gab</td>\n","      <td>1</td>\n","      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n","      <td>the jewish globalist elite have only imported ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20148 rows √ó 4 columns</p>\n","</div>"],"text/plain":["                           post_id  annotators  \\\n","0      1179055004553900032_twitter           0   \n","1      1179063826874032128_twitter           0   \n","2      1178793830532956161_twitter           0   \n","3      1179088797964763136_twitter           1   \n","4      1179085312976445440_twitter           1   \n","...                            ...         ...   \n","20143                  9989999_gab           2   \n","20144                  9990225_gab           2   \n","20145                  9991681_gab           2   \n","20146                  9992513_gab           1   \n","20147                  9998729_gab           1   \n","\n","                                              rationales  \\\n","0                                                     []   \n","1                                                     []   \n","2                                                     []   \n","3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n","...                                                  ...   \n","20143  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...   \n","20144  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n","20145                                                 []   \n","20146  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...   \n","20147  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n","\n","                                             post_tokens  \n","0      i dont think im getting my baby them white he ...  \n","1      we cannot continue calling ourselves feminists...  \n","2                          nawt yall niggers ignoring me  \n","3      i am bit confused coz chinese ppl can not acce...  \n","4      this bitch in whataburger eating a burger with...  \n","...                                                  ...  \n","20143  if ur still on twitter tell carlton i said his...  \n","20144  when i first got on here and said i hate trump...  \n","20145  was macht der moslem wenn der zion gegen seine...  \n","20146  it is awful look at world demographics asians ...  \n","20147  the jewish globalist elite have only imported ...  \n","\n","[20148 rows x 4 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Text preprocessing\n","data['post_tokens'] = data['post_tokens'].apply(lambda x: \" \".join(x)) # comvert token to a compelte sentence\n","data['post_tokens'] = data['post_tokens'].apply(lambda x: x.replace(\"<user>\", \"\")) # remove <user> tag\n","data['post_tokens']= data['post_tokens'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n","data['post_tokens'] = data['post_tokens'].apply(lambda x: p.clean(x)) # emoji removal\n","\n","# Changing label into neumeric value, normal = 0, hatespeech=1, offensive=2\n","data['annotators'] = data['annotators'].apply((lambda x: x[0]['label']))\n","data['annotators'] = data['annotators'] .apply((lambda x: re.sub('normal','0',x)))\n","data['annotators'] = data['annotators'] .apply((lambda x: re.sub('hatespeech','1',x)))\n","data['annotators'] = data['annotators'] .apply((lambda x: re.sub('offensive','2',x)))\n","data['annotators'] =pd.to_numeric(data['annotators'])\n","data"]},{"cell_type":"code","execution_count":null,"id":"fb99382e","metadata":{"id":"fb99382e"},"outputs":[],"source":["#split the data into training and validation sets\n","train_x, valid_x, train_y, valid_y = train_test_split(data['post_tokens'], data['annotators'], test_size=0.05, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"1012fc7e","metadata":{"id":"1012fc7e","outputId":"4072f2ae-b194-42bf-9568-810de7c7ebe0"},"outputs":[{"data":{"text/plain":["9970     1\n","10389    0\n","5485     0\n","5848     0\n","9671     0\n","        ..\n","9653     0\n","11852    2\n","18606    1\n","16782    2\n","8144     0\n","Name: annotators, Length: 1008, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["valid_y"]},{"cell_type":"code","execution_count":null,"id":"28f6be37","metadata":{"id":"28f6be37","outputId":"68eb4441-7ebe-427a-a703-38471740da7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["train comments length:  19140\n","test comments length:  1008\n"]}],"source":["print('train comments length: ',len(train_x))\n","print('test comments length: ',len(valid_x))"]},{"cell_type":"code","execution_count":null,"id":"620f3158","metadata":{"id":"620f3158","outputId":"9a37beac-8703-4109-b934-07db4f669c48"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"]}],"source":["# create a count vectorizer object \n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'.')\n","count_vect.fit(train_x)\n","\n","\n","# word level tf-idf\n","tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n","tfidf_vect.fit(train_x)\n","xtrain_tfidf =  tfidf_vect.transform(train_x)\n","xvalid_tfidf =  tfidf_vect.transform(valid_x)\n","\n","# word level ngram level tf-idf \n","tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n","tfidf_vect_ngram.fit(train_x)\n","xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n","xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n","\n","# characters level tf-idf\n","tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram_chars.fit(train_x)\n","xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n","xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"]},{"cell_type":"code","execution_count":null,"id":"96041194","metadata":{"id":"96041194"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e870e377","metadata":{"id":"e870e377"},"outputs":[],"source":["#non deep elarning model\n","def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n","    # fit the training dataset on the classifier\n","    classifier.fit(xtrain, ytrain)\n","    \n","    # predict the labels on validation dataset\n","    predictions = classifier.predict(xvalid)   \n","    accuracy = metrics.accuracy_score(predictions, yvalid)\n","    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n","    return accuracy, f1score"]},{"cell_type":"code","execution_count":null,"id":"84fb97ee","metadata":{"id":"84fb97ee","outputId":"6c87009c-21d3-43fb-831f-221a5d452a34"},"outputs":[{"name":"stdout","output_type":"stream","text":["NB, WordLevel TF-IDF:   accuracy: 0.5347222222222222     f1 score: 0.4910772067523765\n","NB, N-Gram Vectors:   accuracy: 0.4126984126984127     f1 score: 0.36137777154590073\n","NB, CharLevel Vectors:   accuracy: 0.5426587301587301   f1 score: 0.508499491479479\n"]}],"source":["# Naive Bayes on Word Level TF IDF Vectors\n","accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n","print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n","\n","# Naive Bayes on Ngram Level TF IDF Vectors\n","accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n","print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n","\n","# Naive Bayes on Character Level TF IDF Vectors\n","accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n","print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"]},{"cell_type":"code","execution_count":null,"id":"589f6388","metadata":{"id":"589f6388","outputId":"0d1f48aa-9650-498e-846c-bc934a40078f"},"outputs":[{"name":"stdout","output_type":"stream","text":["LR, CharLevel Vectors:   accuracy: 0.5753968253968254   f1 score: 0.561559887926066\n"]},{"name":"stderr","output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["# Linear Classifier on Character Level TF IDF Vectors\n","accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n","print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"]},{"cell_type":"code","execution_count":null,"id":"47c1bcd9","metadata":{"id":"47c1bcd9"},"outputs":[],"source":["# load the FastTexT pre-trained word-embedding vectors \n","embeddings_index = {}\n","for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n","    values = line.split()\n","    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n","\n","# create a tokenizer\n","token = text.Tokenizer()\n","token.fit_on_texts(data['post_tokens'])\n","word_index = token.word_index\n","\n","# convert text to sequence of tokens and pad them to ensure e\n","# qual length vectors \n","train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n","valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n","\n","# create token-embedding mapping\n","embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"id":"21cd9d92","metadata":{"id":"21cd9d92"},"outputs":[],"source":["# convert to one_hot\n","train_y_onehot = keras.utils.np_utils.to_categorical(train_y, 3)\n","valid_y_onehot = keras.utils.np_utils.to_categorical(valid_y, 3)"]},{"cell_type":"code","execution_count":null,"id":"2ce7713e","metadata":{"id":"2ce7713e"},"outputs":[],"source":["def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 10):\n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","\n","    # Add the convolutional Layer\n","    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n","\n","    # Add the pooling Layer\n","    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n","    model.fit(xtrain, ytrain,\n","              batch_size=256,\n","              epochs=epochs)\n","    predictions = model.predict(xvalid)\n","    predictions = predictions.argmax(axis=-1)\n","    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n","    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n","    return accuracy, f1score"]},{"cell_type":"code","execution_count":null,"id":"e95d2ac2","metadata":{"id":"e95d2ac2","outputId":"f2e9edc6-3b59-47b9-bf80-6c687a137a0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","75/75 [==============================] - 13s 59ms/step - loss: 1.0448 - accuracy: 0.4625\n","Epoch 2/10\n","75/75 [==============================] - 5s 68ms/step - loss: 0.9151 - accuracy: 0.5796\n","Epoch 3/10\n","75/75 [==============================] - 6s 78ms/step - loss: 0.8760 - accuracy: 0.6098\n","Epoch 4/10\n","75/75 [==============================] - 6s 78ms/step - loss: 0.8446 - accuracy: 0.6223\n","Epoch 5/10\n","75/75 [==============================] - 6s 78ms/step - loss: 0.8258 - accuracy: 0.6340\n","Epoch 6/10\n","75/75 [==============================] - 6s 78ms/step - loss: 0.8052 - accuracy: 0.6462\n","Epoch 7/10\n","75/75 [==============================] - 6s 84ms/step - loss: 0.7802 - accuracy: 0.6623\n","Epoch 8/10\n","75/75 [==============================] - 6s 83ms/step - loss: 0.7587 - accuracy: 0.6687\n","Epoch 9/10\n","75/75 [==============================] - 6s 81ms/step - loss: 0.7188 - accuracy: 0.6932\n","Epoch 10/10\n","75/75 [==============================] - 6s 80ms/step - loss: 0.7025 - accuracy: 0.6983\n","CNN, Word Embeddings acuuracy accuracy:0.5803571343421936     f1 score: 0.5837720396480741\n"]}],"source":["accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n","print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}