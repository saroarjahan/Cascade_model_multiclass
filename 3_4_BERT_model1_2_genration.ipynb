{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jpd_rzTUyc9c","outputId":"64cfda32-6d66-4d42-887e-8e8dc7c0f561","executionInfo":{"status":"ok","timestamp":1683958111127,"user_tz":-180,"elapsed":12322,"user":{"displayName":"saroar jahan","userId":"12276237436924535627"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from sklearn.metrics import classification_report\n","from abc import ABC\n"],"metadata":{"id":"mxDQPEsbyZVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uL7GOr50f0YI"},"outputs":[],"source":["class hateLabels(Dataset, ABC):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","class hateModel(ABC):\n","    def __init__(self, model_name='bert-base-uncased', lr=1e-5):\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","        self.model = BertForSequenceClassification.from_pretrained(model_name).to(self.device)\n","        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n","        self.model.train()\n","\n","    def tokenize(self, text):\n","        return self.tokenizer(text.tolist(), truncation=True, padding=True, max_length=128)\n","\n","    def create_data_loader(self, encodings, labels, batch_size=16):\n","        dataset = hateLabels(encodings, labels)\n","        return DataLoader(dataset, batch_size=batch_size)\n","\n","    def train(self, train_texts, train_labels, model_name='model.pt', val_texts=None, val_labels=None, epochs=5):\n","        train_encodings = self.tokenize(train_texts)\n","        train_loader = self.create_data_loader(train_encodings, train_labels)\n","        val_loader = None\n","\n","        if val_texts is not None and val_labels is not None:\n","            val_encodings = self.tokenize(val_texts)\n","            val_loader = self.create_data_loader(val_encodings, val_labels)\n","\n","        for epoch in range(epochs):\n","            print(f'Epoch {epoch+1}/{epochs}')\n","            print('-' * 10)\n","\n","            train_loss = 0\n","            val_loss = 0\n","\n","            # Train the model\n","            for batch in train_loader:\n","                batch = {k: v.to(self.device) for k, v in batch.items()}\n","                outputs = self.model(**batch)\n","                loss = outputs[0]\n","                loss.backward()\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","                train_loss += loss.item()\n","\n","            self.model.eval()\n","\n","            # Validate the model\n","            if val_loader:\n","                for batch in val_loader:\n","                    with torch.no_grad():\n","                        batch = {k: v.to(self.device) for k, v in batch.items()}\n","                        outputs = self.model(**batch)\n","                        loss = outputs[0]\n","                        val_loss += loss.item()\n","\n","                print(f'Train loss {train_loss / len(train_loader)}')\n","                print(f'Validation loss {val_loss / len(val_loader)}')\n","            else:\n","                print(f'Train loss {train_loss / len(train_loader)}')\n","\n","            self.model.train()\n","\n","        # torch.save(self.model.state_dict(), model_name)\n","        # To save:\n","        torch.save({\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'loss': loss,},  model_name)\n","\n","    def evaluate(self, test_texts, test_labels):\n","      self.model.eval()\n","      test_encodings = self.tokenize(test_texts)\n","      test_loader = self.create_data_loader(test_encodings, test_labels)\n","      predictions = []\n","      actuals = []\n","\n","      with torch.no_grad():\n","          for batch in test_loader:\n","              batch = {k: v.to(self.device) for k, v in batch.items()}\n","              outputs = self.model(**batch)\n","              _, preds = torch.max(outputs.logits, dim=1)\n","              predictions.extend(preds.tolist())\n","              actuals.extend(batch['labels'].tolist())\n","\n","      print(classification_report(actuals, predictions))\n","\n"]},{"cell_type":"code","source":["# import  data for model 1\n","train_m1 = pd.read_csv('/content/train_m1.csv')\n","valid_m1 = pd.read_csv('/content/valid_m1.csv')\n","test_m1 = pd.read_csv('/content/test_m1.csv')\n","\n","# import  data for model 2\n","train_m2 = pd.read_csv('/content/train_m2.csv')\n","valid_m2 = pd.read_csv('/content/valid_m2.csv')\n","test_m2 = pd.read_csv('/content/test_m2.csv')\n"],"metadata":{"id":"jNrBULYRyXkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hate_model1 = hateModel()\n","hate_model1.train(train_m1['review'], train_m1['label'], 'model_1.pt',valid_m1['review'], valid_m1['label'])\n","hate_model1.evaluate(test_m1['review'], test_m1['label'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gzF3zCJ0cIy","outputId":"fcd9c513-5bdb-4fa0-ce34-4b83652f356f","executionInfo":{"status":"ok","timestamp":1683963520383,"user_tz":-180,"elapsed":1635758,"user":{"displayName":"saroar jahan","userId":"12276237436924535627"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","----------\n","Train loss 0.5610729292449024\n","Validation loss 0.4674145282506943\n","Epoch 2/5\n","----------\n","Train loss 0.46602758460692945\n","Validation loss 0.3773753650188446\n","Epoch 3/5\n","----------\n","Train loss 0.36253470417836475\n","Validation loss 0.299565083861351\n","Epoch 4/5\n","----------\n","Train loss 0.24339015863554936\n","Validation loss 0.2604717476069927\n","Epoch 5/5\n","----------\n","Train loss 0.16901933113648926\n","Validation loss 0.2248575918376446\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92       812\n","           1       0.94      0.95      0.95      1188\n","\n","    accuracy                           0.94      2000\n","   macro avg       0.93      0.93      0.93      2000\n","weighted avg       0.93      0.94      0.93      2000\n","\n"]}]},{"cell_type":"code","source":["hate_model2 = hateModel()\n","hate_model2.train(train_m2['review'], train_m2['label'], 'model_2.pt',valid_m2['review'], valid_m2['label'])\n","hate_model2.evaluate(test_m2['review'], test_m2['label'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVRJO9bvVgmi","outputId":"cb804280-f7dd-4a45-c53d-7e224931876b","executionInfo":{"status":"ok","timestamp":1683964531876,"user_tz":-180,"elapsed":1011496,"user":{"displayName":"saroar jahan","userId":"12276237436924535627"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","----------\n","Train loss 0.5566032426595688\n","Validation loss 0.6057542969310095\n","Epoch 2/5\n","----------\n","Train loss 0.4788188274741173\n","Validation loss 0.605188672031675\n","Epoch 3/5\n","----------\n","Train loss 0.39160174690485\n","Validation loss 0.6852821419163356\n","Epoch 4/5\n","----------\n","Train loss 0.2745631692767143\n","Validation loss 0.8466342342278314\n","Epoch 5/5\n","----------\n","Train loss 0.17097576160877942\n","Validation loss 1.1309309658550082\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.49      0.52       362\n","           1       0.70      0.75      0.72       574\n","\n","    accuracy                           0.65       936\n","   macro avg       0.63      0.62      0.62       936\n","weighted avg       0.64      0.65      0.65       936\n","\n"]}]}]}